{
	"jobConfig": {
		"name": "csv-cleaning-job",
		"description": "",
		"role": "arn:aws:iam::242201271328:role/GlueDataBrewRole_1",
		"command": "glueetl",
		"version": "5.0",
		"runtime": null,
		"workerType": "G.4X",
		"numberOfWorkers": 5,
		"maxCapacity": 20,
		"jobRunQueuingEnabled": false,
		"maxRetries": 0,
		"timeout": 480,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "csv-cleaning-job.py",
		"scriptLocation": "s3://aws-glue-assets-242201271328-ap-south-1/scripts/",
		"language": "python-3",
		"spark": true,
		"sparkConfiguration": "standard",
		"jobParameters": [],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2025-08-13T05:50:46.881Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://aws-glue-assets-242201271328-ap-south-1/temporary/",
		"glueHiveMetastore": true,
		"etlAutoTuning": true,
		"metrics": true,
		"observabilityMetrics": true,
		"bookmark": "job-bookmark-disable",
		"sparkPath": "s3://aws-glue-assets-242201271328-ap-south-1/sparkHistoryLogs/",
		"flexExecution": false,
		"minFlexWorkers": null,
		"maintenanceWindow": null
	},
	"hasBeenSaved": false,
	"usageProfileName": null,
	"script": "import sys\r\nimport os\r\nimport boto3\r\nimport time\r\nimport json\r\nfrom awsglue.utils import getResolvedOptions\r\nfrom pyspark.context import SparkContext\r\nfrom awsglue.context import GlueContext\r\nfrom awsglue.job import Job\r\nfrom pyspark.sql.functions import col\r\n\r\nargs = getResolvedOptions(sys.argv, ['JOB_NAME', 'S3_INPUT_FILE'])\r\nfilename = os.path.basename(args['S3_INPUT_FILE']).split('.')[0]\r\n\r\nsc = SparkContext()\r\nglueContext = GlueContext(sc)\r\nspark = glueContext.spark_session\r\njob = Job(glueContext)\r\njob.init(args['JOB_NAME'], args)\r\n\r\ndf = spark.read.option(\"header\", \"true\").csv(args['S3_INPUT_FILE'])\r\n\r\n# Drop empty rows and columns\r\ndf = df.dropna(how='all')\r\nnon_empty_cols = [c for c in df.columns if df.filter(col(c).isNotNull()).count() > 0]\r\ndf = df.select(non_empty_cols)\r\n\r\nif df.rdd.isEmpty():\r\n    print(\"⚠️ No data to save after cleaning.\")\r\nelse:\r\n    df = df.coalesce(1)\r\n\r\n    s3_temp_path = f\"s3://nexus-web-frontend1/temp-cleaned/{filename}/\"\r\n    s3_final_path = f\"s3://nexus-web-frontend1/cleaned-output/{filename}.csv\"\r\n\r\n    # Write to temp folder\r\n    df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(s3_temp_path)\r\n    print(f\"✅ Written to temp folder: {s3_temp_path}\")\r\n\r\n    # Rename part file to filename.csv\r\n    s3 = boto3.client('s3')\r\n    bucket = \"nexus-web-frontend1\"\r\n    prefix = f\"temp-cleaned/{filename}/\"\r\n\r\n    objects = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)\r\n    part_file = None\r\n    for obj in objects.get('Contents', []):\r\n        key = obj['Key']\r\n        if key.endswith('.csv'):\r\n            part_file = key\r\n            break\r\n\r\n    if part_file:\r\n        copy_source = {'Bucket': bucket, 'Key': part_file}\r\n        s3.copy_object(Bucket=bucket, CopySource=copy_source, Key=f\"cleaned-output/{filename}.csv\")\r\n        s3.delete_object(Bucket=bucket, Key=part_file)\r\n        print(f\"✅ Renamed and moved to: s3://{bucket}/cleaned-output/{filename}.csv\")\r\n    else:\r\n        print(\"❌ No CSV part file found in temp folder!\")\r\n\r\n# Trigger Lambda\r\nlambda_client = boto3.client('lambda', region_name='ap-south-1')\r\ntime.sleep(10)\r\n\r\npayload = {\r\n    \"detail\": {\r\n        \"crawlerName\": filename\r\n    }\r\n}\r\n\r\ntry:\r\n    response = lambda_client.invoke(\r\n        FunctionName=\"arn:aws:lambda:ap-south-1:242201271328:function:LambdaTriggerQuicksight\",\r\n        InvocationType=\"Event\",\r\n        Payload=json.dumps(payload)\r\n    )\r\n    print(\"✅ Lambda invoked:\", response['StatusCode'])\r\nexcept Exception as e:\r\n    print(\"❌ Lambda error:\", str(e))\r\n\r\njob.commit()\r\n"
}